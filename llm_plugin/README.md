# LLM Plugin

> Optional plugin that explains validation errors and suggests fixes using an LLM backend.

The core CLI runs without this plugin. Install it when you want AI-powered error explanations and design analysis.

---

## Installation

```bash
# LLM plugin only
uv sync --extra llm

# Everything (recommended)
uv sync --all-extras
```

---

## Quick Setup

### 1. Choose a backend

Set `LLM_BACKEND` environment variable:

| Backend | Description |
|---------|-------------|
| `template` | Default. Returns formatted prompts (for testing/debugging). **Echoes the prompt** that would be sent to a real model (prefixed with `[LLM TEMPLATE]`), so you can verify inputs without an API call. |
| `http` / `openai` | OpenAI-compatible API (requires API key) |
| `local` | Placeholder for on-device models |

### 2. Configure via .env file (recommended)

Copy the example environment file and customize it:

```bash
# Copy the template
cp llm_plugin/.env.example llm_plugin/.env

# Edit .env with your credentials
```

**For ChatGPT/OpenAI:**

```bash
LLM_BACKEND=http
OPENAI_API_KEY=sk-your-actual-key-here
# Optional: OPENAI_MODEL=gpt-4o-mini
```

**For Z.AI GLM (Zhipu AI):**

```bash
LLM_BACKEND=http
OPENAI_API_KEY=your-zhipu-api-key
OPENAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
OPENAI_MODEL=glm-4
```

### 3. Alternative: Set environment variables directly

```bash
# Linux/macOS
export OPENAI_API_KEY="sk-..."
export LLM_BACKEND="http"

# Windows PowerShell
$env:OPENAI_API_KEY = "sk-..."
$env:LLM_BACKEND = "http"
```

---

## CLI Usage

### Integrated with core CLI

```bash
uv run pcb-render boards/board.json -o out/board.svg --llm-explain
uv run pcb-render boards/board.json -o out/board.svg --llm-suggest-fixes
uv run pcb-render boards/board.json -o out/board.svg --llm-analyze
```

Combine flags as needed:

```bash
uv run pcb-render boards/board_theta.json -o out/theta.svg --llm-explain --llm-suggest-fixes --permissive
```

### Standalone module invocation

```bash
uv run python -m llm_plugin --help
uv run python -m llm_plugin explain out/board.export.json
uv run python -m llm_plugin suggest-fixes out/board.export.json
uv run python -m llm_plugin analyze out/board.export.json
```

---

## Environment Variables

| Variable | Purpose | Default |
|----------|---------|---------|
| `LLM_BACKEND` | Backend selector | `template` |
| `OPENAI_API_KEY` | API key for OpenAI-compatible endpoints | — |
| `OPENAI_BASE_URL` | Custom API endpoint (e.g., Azure OpenAI) | — |
| `OPENAI_MODEL` | Model name | `gpt-4o-mini` |

### Fallback precedence

- **API key:** `OPENAI_API_KEY` → `PCB_RENDERER_LLM_API_KEY`
- **Base URL:** `OPENAI_BASE_URL` → `LLM_API_BASE` → `PCB_RENDERER_LLM_BASE_URL`

---

## Export JSON Behavior

- Use `--export-json PATH` to save the payload consumed by the plugin.
- If LLM flags are used without `--export-json`, a temporary export is generated automatically.

```bash
# Explicit export
uv run pcb-render boards/board.json -o out/board.svg --export-json out/board.export.json

# Then analyze it standalone
uv run python -m llm_plugin analyze out/board.export.json
```

---

## What the plugin consumes

Export JSON generated by core CLI `_build_export_payload` containing:
- `parse_result`: success flag, errors, normalized board JSON, stats.
- `validation_result`: errors with context, checks_run, counts.
- `render_result`: success flag, output path/format.

## Commands (Typer CLI)

- `explain <export.json>`: summarize validation errors in plain language.
- `suggest-fixes <export.json>`: propose concrete JSON edits based on errors and board context.
- `analyze <export.json>`: share design insights from stats (area, density, trace length, vias).

---

## Notes

- The core CLI auto-registers plugin flags when `llm_plugin` is installed in the environment.
- When LLM flags are used without `--export-json`, a temporary export is created and passed to the plugin.
- Rendering is headless (Matplotlib Agg) for CI compatibility.
