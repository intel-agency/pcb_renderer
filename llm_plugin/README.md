# LLM Plugin

> Optional plugin that explains validation errors and suggests fixes using an LLM backend.

The core CLI runs without this plugin. Install it when you want AI-powered error explanations and design analysis.

---

## Installation

```bash
# LLM plugin only
uv sync --extra llm

# Everything (recommended)
uv sync --all-extras
```

---

## Quick Setup

### 1. Choose a backend

Set `PCBR_LLM_BACKEND` (or `LLM_BACKEND`) environment variable:

| Backend | Description |
| --- | --- |
| `template` | Default. Returns formatted prompts (for testing/debugging). **Echoes the prompt** that would be sent to a real model (prefixed with `[LLM TEMPLATE]`), so you can verify inputs without an API call. |
| `http` / `openai` | OpenAI-compatible API (requires API key) |
| `local` | Placeholder for on-device models |

### 2. Configure via .env file (recommended)

Copy the example environment file and customize it:

```bash
# Copy the template
cp llm_plugin/.env.example llm_plugin/.env

# Edit .env with your credentials
```

**For ChatGPT/OpenAI:**

```bash
PCBR_LLM_BACKEND=http
PCBR_OPENAI_API_KEY=sk-your-actual-key-here
# Optional: PCBR_OPENAI_MODEL=gpt-4o-mini
```

**For Z.AI GLM (Zhipu AI):**

```bash
PCBR_LLM_BACKEND=http
PCBR_OPENAI_API_KEY=your-zhipu-api-key
PCBR_OPENAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
PCBR_OPENAI_MODEL=glm-4
```

### 3. Alternative: Set environment variables directly

```bash
# Linux/macOS
export PCBR_OPENAI_API_KEY="sk-..."
export PCBR_LLM_BACKEND="http"

# Windows PowerShell
$env:PCBR_OPENAI_API_KEY = "sk-..."
$env:PCBR_LLM_BACKEND = "http"

# Or use standard OPENAI_* vars (works with other tools too)
export OPENAI_API_KEY="sk-..."
```

---

## CLI Usage

### Integrated with core CLI

```bash
uv run pcb-render boards/board.json -o out/board.svg --llm-explain
uv run pcb-render boards/board.json -o out/board.svg --llm-suggest-fixes
uv run pcb-render boards/board.json -o out/board.svg --llm-analyze
```

Combine flags as needed:

```bash
uv run pcb-render boards/board_theta.json -o out/theta.svg --llm-explain --llm-suggest-fixes --permissive
```

### Standalone module invocation

```bash
uv run python -m llm_plugin --help
uv run python -m llm_plugin explain out/board.export.json
uv run python -m llm_plugin suggest-fixes out/board.export.json
uv run python -m llm_plugin analyze out/board.export.json
```

---

## Environment Variables

**Variable precedence:** `PCBR_*` project-specific variables override standard `OPENAI_*` variables. This allows per-project configuration while respecting system-wide settings.

| Variable | Purpose | Default |
| --- | --- | --- |
| `PCBR_LLM_BACKEND` / `LLM_BACKEND` | Backend selector | `template` |
| `PCBR_OPENAI_API_KEY` / `OPENAI_API_KEY` | API key for OpenAI-compatible endpoints | — |
| `PCBR_OPENAI_BASE_URL` / `OPENAI_BASE_URL` | Custom API endpoint (e.g., Azure OpenAI, Z.AI GLM) | — |
| `PCBR_OPENAI_MODEL` / `OPENAI_MODEL` | Model name | `gpt-4o-mini` |

**Why two sets?** The `OPENAI_*` variables are standard across many tools (OpenAI SDK, LangChain, etc.). If you already have them configured system-wide, they'll work automatically. Use `PCBR_*` prefixes to override for this project only.

---

## Export JSON Behavior

- Use `--export-json PATH` to save the payload consumed by the plugin.
- If LLM flags are used without `--export-json`, a temporary export is generated automatically.

```bash
# Explicit export
uv run pcb-render boards/board.json -o out/board.svg --export-json out/board.export.json

# Then analyze it standalone
uv run python -m llm_plugin analyze out/board.export.json
```

---

## What the plugin consumes

Export JSON generated by core CLI `_build_export_payload` containing:

- `parse_result`: success flag, errors, normalized board JSON, stats.
- `validation_result`: errors with context, checks_run, counts.
- `render_result`: success flag, output path/format.

## Commands (Typer CLI)

- `explain <export.json>`: summarize validation errors in plain language.
- `suggest-fixes <export.json>`: propose concrete JSON edits based on errors and board context.
- `analyze <export.json>`: share design insights from stats (area, density, trace length, vias).

---

## Notes

- The core CLI auto-registers plugin flags when `llm_plugin` is installed in the environment.
- When LLM flags are used without `--export-json`, a temporary export is created and passed to the plugin.
- Rendering is headless (Matplotlib Agg) for CI compatibility.
