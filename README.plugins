# LLM plugin

Optional plugin that explains validation errors and suggests fixes using an LLM backend. The core CLI runs without it.

Install

```bash
uv sync --extra llm
```

Or install everything:

```bash
uv sync --all-extras
```

Quick setup

- Set backend selector (defaults to `template`):

```bash
$env:LLM_BACKEND="template"
```

- For HTTP/OpenAI-compatible backends, set an API key:

```bash
$env:OPENAI_API_KEY="your-key"
```

CLI usage

```bash
uv run pcb-render boards/board.json -o out/board.svg --llm-explain
uv run pcb-render boards/board.json -o out/board.svg --llm-suggest-fixes --llm-analyze
```

Direct module invocation

```bash
uv run python -m llm_plugin --help
uv run python -m llm_plugin explain export.json
uv run python -m llm_plugin suggest-fixes export.json
uv run python -m llm_plugin analyze export.json
```

Backend environment variables

- `LLM_BACKEND`: `template` (default), `local`, `http`, `openai`
- API key precedence: `OPENAI_API_KEY` → `PCB_RENDERER_LLM_API_KEY`
- Base URL precedence: `OPENAI_BASE_URL` → `LLM_API_BASE` → `PCB_RENDERER_LLM_BASE_URL`
- Model name: `OPENAI_MODEL` (default: `gpt-4o-mini`)

Export JSON behavior

- `--export-json` writes the payload consumed by the plugin.
- If LLM flags are used without `--export-json`, a temporary export is generated automatically.
